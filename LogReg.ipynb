{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37l6IvDp1MXi"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vectorizers\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Gensim\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Transformers / sentence embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Optional: stopwords\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    from nltk.corpus import stopwords\n",
        "    EN_STOPWORDS = set(stopwords.words('english'))\n",
        "except Exception:\n",
        "    EN_STOPWORDS = set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e41175a9"
      },
      "source": [
        "The `FileNotFoundError` suggests that the dataset is not available at the specified `DATA_DIR`. Assuming this is the `aclImdb` dataset, we will download and extract it to the Colab environment. The original path `'/Users/tugs-erdene/Desktop/ЭХБ/models/aclImdb'` seems to be a local path on your machine. We will update `DATA_DIR` to point to the newly downloaded dataset location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e74d5c7"
      },
      "source": [
        "# Download the aclImdb dataset\n",
        "!wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11c176e5"
      },
      "source": [
        "DATA_DIR = \"/content/aclImdb\"\n",
        "\n",
        "def read_reviews(base_dir):\n",
        "    def read_folder(path, label):\n",
        "        texts = []\n",
        "        labels = []\n",
        "        for fname in os.listdir(path):\n",
        "            fpath = os.path.join(path, fname)\n",
        "            if os.path.isfile(fpath):\n",
        "                with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    texts.append(f.read())\n",
        "                    labels.append(label)\n",
        "        return texts, labels\n",
        "\n",
        "    train_pos, y_train_pos = read_folder(os.path.join(base_dir, \"train\", \"pos\"), 1)\n",
        "    train_neg, y_train_neg = read_folder(os.path.join(base_dir, \"train\", \"neg\"), 0)\n",
        "    test_pos, y_test_pos = read_folder(os.path.join(base_dir, \"test\", \"pos\"), 1)\n",
        "    test_neg, y_test_neg = read_folder(os.path.join(base_dir, \"test\", \"neg\"), 0)\n",
        "\n",
        "    X_train = train_pos + train_neg\n",
        "    y_train = y_train_pos + y_train_neg\n",
        "    X_test = test_pos + test_neg\n",
        "    y_test = y_test_pos + y_test_neg\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "X_train_raw, y_train, X_test_raw, y_test = read_reviews(DATA_DIR)\n",
        "print(f\"Loaded: train={len(X_train_raw)}, test={len(X_test_raw)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic text cleaning\n",
        "HTML_RE = re.compile(r\"<.*?>\")\n",
        "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "def clean_text(text, lowercase=True, remove_html=True, remove_punct=True, stopword_removal=False):\n",
        "    if remove_html:\n",
        "        text = HTML_RE.sub(\" \", text)\n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "    if remove_punct:\n",
        "        text = text.translate(PUNCT_TABLE)\n",
        "    tokens = text.split()\n",
        "    if stopword_removal and EN_STOPWORDS:\n",
        "        tokens = [t for t in tokens if t not in EN_STOPWORDS]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply cleaning (toggle stopword_removal=True if desired)\n",
        "X_train = [clean_text(t, stopword_removal=False) for t in X_train_raw]\n",
        "X_test = [clean_text(t, stopword_removal=False) for t in X_test_raw]\n"
      ],
      "metadata": {
        "id": "lQyFgK7w1_hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shared evaluation helper\n",
        "def evaluate_and_print(name, y_true, y_pred, store_list):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", pos_label=1, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-score:  {f1:.4f}\")\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(cm)\n",
        "    store_list.append({\"Embedding\": name, \"Accuracy\": acc, \"Precision\": precision, \"Recall\": recall, \"F1\": f1})\n"
      ],
      "metadata": {
        "id": "EwcZ_ZH32HDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "# Vectorize with TF (L1-normalized counts)\n",
        "count_vec_tf = CountVectorizer()\n",
        "X_train_counts = count_vec_tf.fit_transform(X_train)\n",
        "X_test_counts = count_vec_tf.transform(X_test)\n",
        "X_train_tf = normalize(X_train_counts.astype(float), norm=\"l1\", axis=1)\n",
        "X_test_tf = normalize(X_test_counts.astype(float), norm=\"l1\", axis=1)\n",
        "\n",
        "# Train Logistic Regression\n",
        "logreg_tf = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_tf.fit(X_train_tf, y_train)\n",
        "y_pred_tf = logreg_tf.predict(X_test_tf)\n",
        "\n",
        "evaluate_and_print(\"TF (term frequency)\", y_test, y_pred_tf, results)\n"
      ],
      "metadata": {
        "id": "fS-Gqnhh2MIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary presence + IDF transform (no normalization)\n",
        "count_vec_bin = CountVectorizer(binary=True)\n",
        "X_train_bin = count_vec_bin.fit_transform(X_train)\n",
        "X_test_bin = count_vec_bin.transform(X_test)\n",
        "\n",
        "idf_transformer = TfidfTransformer(use_idf=True, norm=None)\n",
        "X_train_idf = idf_transformer.fit_transform(X_train_bin)\n",
        "X_test_idf = idf_transformer.transform(X_test_bin)\n",
        "\n",
        "logreg_idf = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_idf.fit(X_train_idf, y_train)\n",
        "y_pred_idf = logreg_idf.predict(X_test_idf)\n",
        "\n",
        "evaluate_and_print(\"IDF only (binary presence × IDF)\", y_test, y_pred_idf, results)\n"
      ],
      "metadata": {
        "id": "GIK4slb-2d4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_uni = TfidfVectorizer(ngram_range=(1,1), min_df=2)\n",
        "X_train_tfidf_uni = tfidf_uni.fit_transform(X_train)\n",
        "X_test_tfidf_uni = tfidf_uni.transform(X_test)\n",
        "\n",
        "logreg_tfidf_uni = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_tfidf_uni.fit(X_train_tfidf_uni, y_train)\n",
        "y_pred_tfidf_uni = logreg_tfidf_uni.predict(X_test_tfidf_uni)\n",
        "\n",
        "evaluate_and_print(\"TF-IDF (unigram)\", y_test, y_pred_tfidf_uni, results)\n"
      ],
      "metadata": {
        "id": "FOTMMDpz2nG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_unibi = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
        "X_train_tfidf_unibi = tfidf_unibi.fit_transform(X_train)\n",
        "X_test_tfidf_unibi = tfidf_unibi.transform(X_test)\n",
        "\n",
        "logreg_tfidf_unibi = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_tfidf_unibi.fit(X_train_tfidf_unibi, y_train)\n",
        "y_pred_tfidf_unibi = logreg_tfidf_unibi.predict(X_test_tfidf_unibi)\n",
        "\n",
        "evaluate_and_print(\"TF-IDF (unigram+bigram)\", y_test, y_pred_tfidf_unibi, results)\n"
      ],
      "metadata": {
        "id": "pPNAzQ5E2xu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize for Word2Vec\n",
        "def tokenize_for_w2v(text):\n",
        "    return text.split()\n",
        "\n",
        "train_tokens = [tokenize_for_w2v(t) for t in X_train]\n",
        "test_tokens = [tokenize_for_w2v(t) for t in X_test]\n",
        "\n",
        "# Train CBOW (sg=0)\n",
        "w2v_cbow = Word2Vec(\n",
        "    sentences=train_tokens,\n",
        "    vector_size=200,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    workers=4,\n",
        "    sg=0,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Build averaged document vectors\n",
        "def doc_vector(tokens, model):\n",
        "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.wv.vector_size, dtype=np.float32)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "X_train_cbow = np.vstack([doc_vector(toks, w2v_cbow) for toks in train_tokens])\n",
        "X_test_cbow = np.vstack([doc_vector(toks, w2v_cbow) for toks in test_tokens])\n",
        "\n",
        "logreg_cbow = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_cbow.fit(X_train_cbow, y_train)\n",
        "y_pred_cbow = logreg_cbow.predict(X_test_cbow)\n",
        "\n",
        "evaluate_and_print(\"Word2Vec CBOW (trained on IMDB)\", y_test, y_pred_cbow, results)\n"
      ],
      "metadata": {
        "id": "LH19q0qB3G0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Skip-gram (sg=1)\n",
        "w2v_sg = Word2Vec(\n",
        "    sentences=train_tokens,\n",
        "    vector_size=200,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    workers=4,\n",
        "    sg=1,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "X_train_sg = np.vstack([doc_vector(toks, w2v_sg) for toks in train_tokens])\n",
        "X_test_sg = np.vstack([doc_vector(toks, w2v_sg) for toks in test_tokens])\n",
        "\n",
        "logreg_sg = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_sg.fit(X_train_sg, y_train)\n",
        "y_pred_sg = logreg_sg.predict(X_test_sg)\n",
        "\n",
        "evaluate_and_print(\"Word2Vec Skip-gram (trained on IMDB)\", y_test, y_pred_sg, results)\n"
      ],
      "metadata": {
        "id": "uVvb9d9R4Yxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose one: 'word2vec-google-news-300' (large) or 'fasttext-wiki-news-subwords-300'\n",
        "# 'glove-wiki-gigaword-300' is another option if the above aren't available.\n",
        "try:\n",
        "    pretrained = api.load(\"word2vec-google-news-300\")  # ~1.5GB\n",
        "    model_name = \"Pretrained Word2Vec (GoogleNews)\"\n",
        "except Exception:\n",
        "    pretrained = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "    model_name = \"Pretrained FastText (WikiNews)\"\n",
        "\n",
        "def doc_vector_pretrained(tokens, keyed_vectors):\n",
        "    vectors = []\n",
        "    for w in tokens:\n",
        "        if w in keyed_vectors:\n",
        "            vectors.append(keyed_vectors[w])\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(keyed_vectors.vector_size, dtype=np.float32)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "X_train_pre = np.vstack([doc_vector_pretrained(toks, pretrained) for toks in train_tokens])\n",
        "X_test_pre = np.vstack([doc_vector_pretrained(toks, pretrained) for toks in test_tokens])\n",
        "\n",
        "logreg_pre = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_pre.fit(X_train_pre, y_train)\n",
        "y_pred_pre = logreg_pre.predict(X_test_pre)\n",
        "\n",
        "evaluate_and_print(model_name, y_test, y_pred_pre, results)\n"
      ],
      "metadata": {
        "id": "QLkcI1Ag7JLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General sentence embeddings (compact and strong)\n",
        "st_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "X_train_st = st_model.encode(X_train, batch_size=64, convert_to_numpy=True, show_progress_bar=True)\n",
        "X_test_st = st_model.encode(X_test, batch_size=64, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "logreg_st = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg_st.fit(X_train_st, y_train)\n",
        "y_pred_st = logreg_st.predict(X_test_st)\n",
        "\n",
        "evaluate_and_print(\"BERT sentence embeddings (all-MiniLM-L6-v2)\", y_test, y_pred_st, results)\n"
      ],
      "metadata": {
        "id": "YbYvvrKH804o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dabert_name = \"textattack/bert-base-uncased-imdb\"\n",
        "dabert_tok = AutoTokenizer.from_pretrained(dabert_name)\n",
        "dabert = AutoModel.from_pretrained(dabert_name).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def bert_embed(texts, tokenizer, model, max_length=128, batch_size=32):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "        model_output = model(**encoded_input)\n",
        "        # Mean pooling: take the average of the last hidden state (token embeddings)\n",
        "        # and apply attention mask to ignore padding tokens\n",
        "        input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(model_output.last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(model_output.last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "    return np.array(all_embeddings)"
      ],
      "metadata": {
        "id": "UHTjNYXNRsX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Таны өгсөн үр дүнгүүдийг жагсаалтад оруулах\n",
        "data = [\n",
        "    {\"Embedding\": \"TF-IDF (unigram+bigram)\", \"Accuracy\": 0.8908, \"F1\": 0.8915},\n",
        "    {\"Embedding\": \"IDF only (binary presence × IDF)\", \"Accuracy\": 0.8734, \"F1\": 0.8725},\n",
        "    {\"Embedding\": \"Word2Vec Skip-gram (IMDB)\", \"Accuracy\": 0.8598, \"F1\": 0.8589},\n",
        "    {\"Embedding\": \"Pretrained Word2Vec (GoogleNews)\", \"Accuracy\": 0.8483, \"F1\": 0.8467},\n",
        "    {\"Embedding\": \"Word2Vec CBOW (IMDB)\", \"Accuracy\": 0.8476, \"F1\": 0.8474},\n",
        "    {\"Embedding\": \"BERT (all-MiniLM-L6-v2)\", \"Accuracy\": 0.8192, \"F1\": 0.8184},\n",
        "    {\"Embedding\": \"TF (term frequency)\", \"Accuracy\": 0.7246, \"F1\": 0.7294}\n",
        "]\n",
        "\n",
        "# 2. DataFrame үүсгэх\n",
        "df_final = pd.DataFrame(data)\n",
        "\n",
        "# 3. 'Embedding' баганыг индекс болгох\n",
        "df_final.set_index('Embedding', inplace=True)\n",
        "\n",
        "# 4. Accuracy-аар нь ихээс бага руу эрэмбэлэх\n",
        "df_final = df_final.sort_values(by='Accuracy', ascending=False)\n",
        "\n",
        "# 5. Хүснэгтийг хэвлэх\n",
        "print(\"=== Final Comparison ===\")\n",
        "print(df_final.to_string(float_format=\"{:.4f}\".format))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEUcmYDpAOFe",
        "outputId": "ef66b72c-e3b0-4e30-82a5-2c26077d613c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Final Comparison ===\n",
            "                                  Accuracy     F1\n",
            "Embedding                                        \n",
            "TF-IDF (unigram+bigram)             0.8908 0.8915\n",
            "IDF only (binary presence × IDF)    0.8734 0.8725\n",
            "Word2Vec Skip-gram (IMDB)           0.8598 0.8589\n",
            "Pretrained Word2Vec (GoogleNews)    0.8483 0.8467\n",
            "Word2Vec CBOW (IMDB)                0.8476 0.8474\n",
            "BERT (all-MiniLM-L6-v2)             0.8192 0.8184\n",
            "TF (term frequency)                 0.7246 0.7294\n"
          ]
        }
      ]
    }
  ]
}