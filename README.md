# üé¨ IMDb Sentiment Analysis

–≠–Ω—ç—Ö“Ø“Ø repository –Ω—å **IMDb –∫–∏–Ω–æ —Ç–æ–π–º–Ω—ã ”©–≥”©–≥–¥”©–ª** –¥—ç—ç—Ä sentiment analysis (—ç–µ—Ä—ç–≥ / —Å”©—Ä”©–≥) —Ö–∏–π—Ö –∑–æ—Ä–∏–ª–≥–æ–æ—Ä **”©”©—Ä ”©”©—Ä embedding –∞—Ä–≥—É—É–¥** –±–æ–ª–æ–Ω **–∞–Ω–≥–∏–ª–∞–≥—á –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Ö–∞—Ä—å—Ü—É—É–ª–∞–Ω —Å—É–¥–∞–ª—Å–∞–Ω —Ç—É—Ä—à–∏–ª—Ç—ã–Ω –∞–∂–ª—ã–≥** –∞–≥—É—É–ª–Ω–∞. –£–ª–∞–º–∂–ª–∞–ª—Ç machine learning –∞—Ä–≥—É—É–¥ –±–æ–ª–æ–Ω –æ—Ä—á–∏–Ω “Ø–µ–∏–π–Ω transformer-–¥ —Å—É—É—Ä–∏–ª—Å–∞–Ω embedding-“Ø“Ø–¥–∏–π–≥ —Å–∏—Å—Ç–µ–º—Ç—ç–π–≥—ç—ç—Ä —Ö–∞—Ä—å—Ü—É—É–ª–∂, —Ç—ç–¥–≥—ç—ç—Ä–∏–π–Ω –¥–∞–≤—É—É –±–∞ —Å—É–ª —Ç–∞–ª—ã–≥ —Ç—É—Ä—à–∏–ª—Ç–∞–∞—Ä “Ø–Ω—ç–ª—Å—ç–Ω –±–æ–ª–Ω–æ.

---

## üß† Abstract

–≠–Ω—ç—Ö“Ø“Ø —Å—É–¥–∞–ª–≥–∞–∞–≥–∞–∞—Ä IMDb –∫–∏–Ω–æ —Ç–æ–π–º–Ω—ã ”©–≥”©–≥–¥”©–ª –¥—ç—ç—Ä sentiment analysis —Ö–∏–π—Ö –∑–æ—Ä–∏–ª–≥–æ–æ—Ä **TF, IDF, TF-IDF, Word2Vec (CBOW, Skip-gram), BERT** embedding-“Ø“Ø–¥–∏–π–≥ **Logistic Regression, Random Forest, AdaBoost, LSTM** –∑—ç—Ä—ç–≥ –∞–Ω–≥–∏–ª–∞–≥—á –∑–∞–≥–≤–∞—Ä—É—É–¥—Ç–∞–π —Ö–æ—Å–ª—É—É–ª–∞–Ω –∞—à–∏–≥–ª–∞–≤. –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–≥ **Stratified K-Fold Cross Validation** –∞—Ä–≥–∞–∞—Ä “Ø–Ω—ç–ª–∂, **Accuracy, Precision, Recall, Macro-F1** “Ø–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥–∏–π–≥ –∞—à–∏–≥–ª–∞–Ω —Ö–∞—Ä—å—Ü—É—É–ª—Å–∞–Ω.

“Æ—Ä –¥“Ø–Ω–≥—ç—ç—Å —Ö–∞—Ä–∞—Ö–∞–¥ TF-IDF embedding –Ω—å classical ML –∑–∞–≥–≤–∞—Ä—É—É–¥—Ç–∞–π —Ö–æ—Å–ª–æ—Ö–æ–¥ —Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π ”©–Ω–¥”©—Ä –≥“Ø–π—Ü—ç—Ç–≥—ç–ª “Ø–∑“Ø“Ø–ª—Å—ç–Ω –±–æ–ª, BERT embedding –Ω—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏–π–Ω –º—ç–¥—ç—ç–ª–ª–∏–π–≥ –∏–ª“Ø“Ø —Å–∞–π–Ω —Ç—É—Å–≥–∞—Å–∞–Ω —á —Ç–æ–æ—Ü–æ–æ–ª–ª—ã–Ω –∑–∞—Ä–¥–∞–ª ”©–Ω–¥”©—Ä, –∑–∞—Ä–∏–º —Ç–æ—Ö–∏–æ–ª–¥–æ–ª–¥ overfitting –∏–ª—ç—Ä—Å—ç–Ω.

---

## üìå Introduction

Sentiment analysis –Ω—å —Ç–µ–∫—Å—Ç—ç–Ω ”©–≥”©–≥–¥–ª”©”©—Å —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Å–∞–Ω–∞–ª –±–æ–¥–æ–ª, —Ö–∞–Ω–¥–ª–∞–≥—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö **Natural Language Processing (NLP)**-–∏–π–Ω —á—É—Ö–∞–ª —Å–∞–ª–±–∞—Ä —é–º. –û–Ω–ª–∞–π–Ω –æ—Ä—á–∏–Ω–¥ —Ö—É—Ä–∏–º—Ç–ª–∞–≥–¥–∞–∂ –±—É–π –∫–∏–Ω–æ, –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω, “Ø–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π —Ç–æ–π–º—É—É–¥—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∞–Ω–≥–∏–ª–∞—Ö —Ö—ç—Ä—ç–≥—Ü—ç—ç —É–ª–∞–º –±“Ø—Ä –Ω—ç–º—ç–≥–¥—ç–∂ –±–∞–π–Ω–∞.

IMDb –∫–∏–Ω–æ —Ç–æ–π–º–Ω—ã dataset –Ω—å sentiment analysis –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–≥ —Ç—É—Ä—à–∏—Ö–∞–¥ ”©—Ä–≥”©–Ω —Ö—ç—Ä—ç–≥–ª—ç–≥–¥–¥—ç–≥ **—Å—Ç–∞–Ω–¥–∞—Ä—Ç benchmark ”©–≥”©–≥–¥”©–ª** –±”©–≥”©”©–¥ —ç–Ω—ç—Ö“Ø“Ø –∞–∂–ª—ã–Ω –∑–æ—Ä–∏–ª–≥–æ –Ω—å —É–≥ dataset –¥—ç—ç—Ä **”©”©—Ä ”©”©—Ä embedding + classifier —Ö–æ—Å–ª–æ–ª—É—É–¥—ã–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ –±–æ–¥–∏—Ç —Ç—É—Ä—à–∏–ª—Ç–∞–∞—Ä —Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö** —è–≤–¥–∞–ª —é–º.

---

## üìä Dataset Description

**Stanford Large Movie Review Dataset (IMDb)**

* –ù–∏–π—Ç —Ç–æ–π–º: **50,000**
* Train set: **25,000**
* Test set: **25,000**
* –ê–Ω–≥–∏–ª–∞–ª: Positive / Negative (50% ‚Äì 50%)
* –•—ç–ª: –ê–Ω–≥–ª–∏
* –û–Ω—Ü–ª–æ–≥: –ë–æ–¥–∏—Ç —Ö—ç—Ä—ç–≥–ª—ç–≥—á–¥–∏–π–Ω –±–∏—á—Å—ç–Ω, —É—Ä—Ç –Ω—å —Ö–∞—Ä–∏–ª—Ü–∞–Ω –∞–¥–∏–ª–≥“Ø–π —Ç–æ–π–º—É—É–¥

üì• Dataset —Ç–∞—Ç–∞—Ö –ª–∏–Ω–∫:
[https://ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/)

---

## üîç Dataset –¥—ç—ç—Ä —Ö–∏–π–≥–¥—Å—ç–Ω task-—É—É–¥

–≠–Ω—ç—Ö“Ø“Ø –∞–∂–ª—ã–Ω —Ö“Ø—Ä—ç—ç–Ω–¥ dataset –¥—ç—ç—Ä –¥–∞—Ä–∞–∞—Ö **–≥–æ–ª task-—É—É–¥—ã–≥ 3 —É–¥–∞–∞–≥–∏–π–Ω —Ç–æ–º–æ–æ—Ö–æ–Ω —Ç—É—Ä—à–∏–ª—Ç–∞–∞—Ä** —Ö–∏–π—Å—ç–Ω:

1. **Text preprocessing –±–∞ feature engineering**
2. **Embedding –∞—Ä–≥—É—É–¥—ã–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç**
3. **Classical ML vs Embedding-based model** —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç
4. **Cross-validation –±“Ø—Ö–∏–π –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–Ω “Ø–Ω—ç–ª–≥—ç—ç**

---

## üß™ Preprocessing

”®–≥”©–≥–¥”©–ª–¥ –¥–∞—Ä–∞–∞—Ö preprocessing –∞–ª—Ö–º—É—É–¥—ã–≥ —Ö—ç—Ä—ç–≥–∂“Ø“Ø–ª—Å—ç–Ω:

* Text lowercase –±–æ–ª–≥–æ—Ö
* Tokenization
* Stopword removal (–∑–∞—Ä–∏–º —Ç—É—Ä—à–∏–ª—Ç–∞–¥)
* TF / IDF / TF-IDF vectorization
* Word2Vec-–¥ sequence “Ø“Ø—Å–≥—ç—Ö
* BERT embedding-–¥ padding & truncation –∞—à–∏–≥–ª–∞—Ö
* Word2Vec embedding-–¥ ”©–≥“Ø“Ø–ª–±—ç—Ä–∏–π–Ω representation-–∏–π–≥ **“Ø–≥“Ø“Ø–¥–∏–π–Ω embedding-–∏–π–Ω –¥—É–Ω–¥–∞–∂** —É—Ç–≥–∞–∞—Ä —Ç–æ–æ—Ü–æ—Ö

---

## üß¨ Embedding –∞—Ä–≥—É—É–¥—ã–Ω —Ç–∞–π–ª–±–∞—Ä

### TF (Term Frequency)

“Æ–≥–∏–π–Ω —Ç—É—Ö–∞–π–Ω –±–∞—Ä–∏–º—Ç –±–∏—á–∏–≥ –¥—ç—Ö –¥–∞–≤—Ç–∞–º–∂–∏–¥ —Å—É—É—Ä–∏–ª—Å–∞–Ω —ç–Ω–≥–∏–π–Ω —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∞—Ä–≥–∞.

### IDF (Inverse Document Frequency)

“Æ–≥ —Ç—É—Ö–∞–π–Ω corpus-–¥ —Ö—ç—Ä —Ö–æ–≤–æ—Ä –±–∞–π–≥–∞–∞–≥ –∏–ª—ç—Ä—Ö–∏–π–ª–Ω—ç:

IDF(t) = log(N / df(t))

### TF-IDF

TF –±–∞ IDF-–∏–π–Ω “Ø—Ä–∂–≤—ç—Ä –±”©–≥”©”©–¥ “Ø–≥–∏–π–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–ª—ã–≥ –∏–ª“Ø“Ø —Å–∞–π–Ω —Ç—É—Å–≥–∞–¥–∞–≥.

### Word2Vec (CBOW, Skip-gram)

–ù–µ–π—Ä–æ–Ω —Å“Ø–ª–∂—ç—ç–≥—ç—ç—Ä “Ø–≥ —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —Å–µ–º–∞–Ω—Ç–∏–∫ —Ö–∞–º–∞–∞—Ä–ª—ã–≥ —Å—É—Ä–¥–∞–≥ embedding –∞—Ä–≥–∞.

* CBOW: Context ‚Üí Word
* Skip-gram: Word ‚Üí Context

### BERT Embedding

Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—Ç —Å—É—É—Ä–∏–ª—Å–∞–Ω, **–∫–æ–Ω—Ç–µ–∫—Å—Ç—ç–¥ –º—ç–¥—Ä—ç–º—Ç–≥–∏–π** representation “Ø“Ø—Å–≥—ç–¥—ç–≥ pretrained –∑–∞–≥–≤–∞—Ä—É—É–¥.

---

## ü§ñ –ê—à–∏–≥–ª–∞—Å–∞–Ω –∞–Ω–≥–∏–ª–∞–≥—á –∑–∞–≥–≤–∞—Ä—É—É–¥

### Logistic Regression

–ê–Ω—Ö **David Cox (1958)** —Ç–∞–Ω–∏–ª—Ü—É—É–ª—Å–∞–Ω, –º–∞–≥–∞–¥–ª–∞–ª–¥ —Å—É—É—Ä–∏–ª—Å–∞–Ω —à—É–≥–∞–º–∞–Ω –∞–Ω–≥–∏–ª–∞–≥—á.

* –ó–æ—Ä–∏–ª–≥–æ: Binary classification
* Loss: Log-loss (Cross-entropy)
* –î–∞–≤—É—É —Ç–∞–ª: –•—É—Ä–¥–∞–Ω, —Ç–∞–π–ª–±–∞—Ä–ª–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä

### Random Forest

Decision Tree-“Ø“Ø–¥–∏–π–Ω –∞–Ω—Å–∞–º–±–ª—å –∑–∞–≥–≤–∞—Ä.

### AdaBoost

–°—É–ª –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–≥ –¥–∞—Ä–∞–∞–ª–∞–Ω —Å–∞–π–∂—Ä—É—É–ª–∂ —Å—É—Ä–≥–∞–¥–∞–≥ boosting –∞—Ä–≥–∞.

### LSTM

Sequence ”©–≥”©–≥–¥”©–ª –¥—ç—ç—Ä —É—Ä—Ç —Ö—É–≥–∞—Ü–∞–∞–Ω—ã —Ö–∞–º–∞–∞—Ä–ª—ã–≥ —Å—É—Ä–∞—Ö —á–∞–¥–≤–∞—Ä—Ç–∞–π RNN.

---

## ‚öôÔ∏è Experimental Setup

* Cross-validation: **Stratified 5-Fold**
* “Æ–Ω—ç–ª–≥—ç—ç–Ω–∏–π “Ø–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥:

  * Accuracy
  * Precision
  * Recall
  * Macro-F1 score

### Hyperparameter tuning:

* Logistic Regression: C
* Random Forest: n_estimators, max_depth
* AdaBoost: n_estimators
* LSTM: embedding size, hidden units, epochs

---

## üñ•Ô∏è –¢—É—Ä—à–∏–ª—Ç—ã–Ω –æ—Ä—á–∏–Ω

* Platform: **Google Colab**
* CPU: Intel Xeon
* GPU: (–∑–∞—Ä–∏–º BERT —Ç—É—Ä—à–∏–ª—Ç–∞–¥)
* RAM: ~12GB
* OS: Linux
* –¢—É—Ä—à–∏–ª—Ç—ã–Ω –¥–∞–≤—Ç–∞–º–∂: **3 “Ø–Ω–¥—Å—ç–Ω —Ç—É—Ä—à–∏–ª—Ç + cross-validation**

---

## üìà Results & Comparison

### üîπ Experiment Set 1

| Embedding       | Accuracy | F1     |
| --------------- | -------- | ------ |
| BERT embeddings | 0.7569   | 0.7548 |
| TF              | 0.7144   | 0.7491 |
| TF-IDF unigram  | 0.7046   | 0.7488 |
| IDF-only        | 0.7112   | 0.7348 |
| Word2Vec CBOW   | 0.7368   | 0.7264 |

### üîπ Experiment Set 2

| Embedding          | Accuracy   | F1         |
| ------------------ | ---------- | ---------- |
| TF-IDF (uni+bi)    | **0.8908** | **0.8915** |
| IDF only           | 0.8734     | 0.8725     |
| Word2Vec Skip-gram | 0.8598     | 0.8589     |
| Word2Vec CBOW      | 0.8476     | 0.8474     |
| BERT (MiniLM)      | 0.8192     | 0.8184     |
| TF                 | 0.7246     | 0.7294     |

### üîπ Experiment Set 3

| Embedding       | Accuracy | Precision | Recall | F1         |
| --------------- | -------- | --------- | ------ | ---------- |
| IDF             | 0.8328   | 0.8106    | 0.8685 | **0.8386** |
| TF-IDF unigram  | 0.8215   | 0.8073    | 0.8447 | 0.8256     |
| TF              | 0.8204   | 0.8078    | 0.8409 | 0.8240     |
| BERT embeddings | 0.7661   | 0.7605    | 0.7770 | 0.7686     |
| Word2Vec CBOW   | 0.7590   | 0.7574    | 0.7620 | 0.7597     |

---

## üí¨ Discussion

* TF-IDF –Ω—å **classical ML-–¥ —Ö–∞–º–≥–∏–π–Ω —Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π baseline** –±–∞–π–≤
* Word2Vec –Ω—å —Å–µ–º–∞–Ω—Ç–∏–∫ –º—ç–¥—ç—ç–ª—ç–ª –∞–≥—É—É–ª—Å–∞–Ω —á classifier-—Ç–∞–π —Ö–æ—Å–ª–æ—Ö–æ–¥ –¥–∞–≤—É—É —Ç–∞–ª –±–∞–≥–∞
* BERT embedding –Ω—å –æ–Ω–æ–ª—ã–Ω —Ö—É–≤—å–¥ —Ö“Ø—á—Ç—ç–π —á:

  * Dataset size
  * Fine-tuning —Ö–∏–π–≥–¥—ç—ç–≥“Ø–π
  * –¢–æ–æ—Ü–æ–æ–ª–ª—ã–Ω —Ö—è–∑–≥–∞–∞—Ä–ª–∞–ª—Ç
    –∑—ç—Ä–≥—ç—ç—Å —à–∞–ª—Ç–≥–∞–∞–ª–∞–Ω –∑–∞—Ä–∏–º —Ç—É—Ä—à–∏–ª—Ç–∞–¥ —Å—É–ª –≥–∞—Ä—Å–∞–Ω

---

## üèÅ Conclusion

Embedding –∞—Ä–≥–∞ –±–æ–ª–æ–Ω classifier-–∏–π–Ω —Å–æ–Ω–≥–æ–ª—Ç –Ω—å sentiment analysis-–∏–π–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–¥ —à—É—É–¥ –Ω”©–ª”©”©–ª–¥”©–≥. –ü—Ä–∞–∫—Ç–∏–∫ —Ö—ç—Ä—ç–≥–ª—ç—ç–Ω–¥ **TF-IDF + Logistic Regression** –Ω—å —Ö—É—Ä–¥, –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–Ω —Ç—ç–Ω—Ü–≤—ç—Ä–∏–π–≥ —Å–∞–π–Ω —Ö–∞–Ω–≥–∞–∂ –±–∞–π—Å–∞–Ω –±–æ–ª, BERT –Ω—å –∏–ª“Ø“Ø –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª—Ç–∞–π –±–æ–ª–æ–≤—á –Ω”©”©—Ü –∏—Ö —à–∞–∞—Ä–¥—Å–∞–Ω.

---

## üîÆ Future Work

* BERT-–∏–π–≥ full fine-tuning —Ö–∏–π—Ö
* Inference —Ö—É—Ä–¥ –±–∞ model size-–∏–π–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç
* Class imbalance –±“Ø—Ö–∏–π ”©–≥”©–≥–¥”©–ª –¥—ç—ç—Ä —Ç—É—Ä—à–∏—Ö

---

## üìö References

IMDb sentiment analysis-—Ç—ç–π —Ö–æ–ª–±–æ–æ—Ç–æ–π **10+ —Å—É–¥–∞–ª–≥–∞–∞–Ω—ã –∞–∂–∏–ª** –∞—à–∏–≥–ª–∞—Å–∞–Ω (README —Ç”©–≥—Å–≥”©–ª–¥ –±“Ø—Ä—ç–Ω –∂–∞–≥—Å–∞–∞–ª—Ç —Ö–∞–≤—Å–∞—Ä–≥–∞—Å–∞–Ω).
