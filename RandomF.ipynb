{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG6p9q74Ynbw"
      },
      "outputs": [],
      "source": [
        "# Download the aclImdb dataset\n",
        "!wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItCv4RTFwVfS"
      },
      "outputs": [],
      "source": [
        "!pip install gensim sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rqH6pjoYAL3"
      },
      "outputs": [],
      "source": [
        "import os, re, string, time, logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "!pip install gensim sentence_transformers\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# -----------------------------\n",
        "# Logging setup\n",
        "# -----------------------------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------------\n",
        "# Load IMDB dataset\n",
        "# -----------------------------\n",
        "DATA_DIR = \"/content/aclImdb\"\n",
        "\n",
        "def read_reviews(base_dir):\n",
        "    def read_folder(path, label):\n",
        "        texts, labels = [], []\n",
        "        for fname in os.listdir(path):\n",
        "            fpath = os.path.join(path, fname)\n",
        "            if os.path.isfile(fpath):\n",
        "                with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    texts.append(f.read())\n",
        "                    labels.append(label)\n",
        "        return texts, labels\n",
        "\n",
        "    train_pos, y_train_pos = read_folder(os.path.join(base_dir, \"train\", \"pos\"), 1)\n",
        "    train_neg, y_train_neg = read_folder(os.path.join(base_dir, \"train\", \"neg\"), 0)\n",
        "    test_pos, y_test_pos = read_folder(os.path.join(base_dir, \"test\", \"pos\"), 1)\n",
        "    test_neg, y_test_neg = read_folder(os.path.join(base_dir, \"test\", \"neg\"), 0)\n",
        "\n",
        "    return train_pos+train_neg, y_train_pos+y_train_neg, test_pos+test_neg, y_test_pos+y_test_neg\n",
        "\n",
        "X_train_raw, y_train, X_test_raw, y_test = read_reviews(DATA_DIR)\n",
        "print(f\"Loaded: train={len(X_train_raw)}, test={len(X_test_raw)}\")\n",
        "# -----------------------------\n",
        "# Preprocessing\n",
        "# -----------------------------\n",
        "HTML_RE = re.compile(r\"<.*?>\")\n",
        "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = HTML_RE.sub(\" \", text)\n",
        "    text = text.lower()\n",
        "    text = text.translate(PUNCT_TABLE)\n",
        "    return text\n",
        "\n",
        "X_train = [clean_text(t) for t in X_train_raw]\n",
        "X_test = [clean_text(t) for t in X_test_raw]\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation helper\n",
        "# -----------------------------\n",
        "def evaluate(name, y_true, y_pred, results):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    logger.info(f\"[{name}] Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}\")\n",
        "    logger.info(f\"[{name}] Confusion matrix:\\n{cm}\")\n",
        "    results.append({\"Embedding\":name,\"Accuracy\":acc,\"Precision\":prec,\"Recall\":rec,\"F1\":f1})\n",
        "\n",
        "def tune_rf(X_train_emb, y_train, name):\n",
        "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    param_grid = {\n",
        "        \"n_estimators\":[50],\n",
        "        \"max_depth\":[10,20],\n",
        "        \"min_samples_split\":[2,5],\n",
        "        \"min_samples_leaf\":[5,10]\n",
        "    }\n",
        "    logger.info(f\"[{name}] Starting GridSearchCV...\")\n",
        "    grid = GridSearchCV(rf, param_grid, scoring=\"f1\", cv=3, n_jobs=-1, verbose=2)\n",
        "    start = time.time()\n",
        "    grid.fit(X_train_emb, y_train)\n",
        "    end = time.time()\n",
        "    logger.info(f\"[{name}] Best params: {grid.best_params_}, Best CV F1={grid.best_score_:.4f}, Duration={(end-start):.2f}s\")\n",
        "    return grid.best_estimator_, grid.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2zQY0ThaaOU"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "count_vec = CountVectorizer()\n",
        "X_train_counts = count_vec.fit_transform(X_train)\n",
        "X_test_counts = count_vec.transform(X_test)\n",
        "X_train_tf = normalize(X_train_counts, norm=\"l1\")\n",
        "X_test_tf = normalize(X_test_counts, norm=\"l1\")\n",
        "\n",
        "rf_tf, params_tf = tune_rf(X_train_tf, y_train, \"TF\")\n",
        "evaluate(\"TF\", y_test, rf_tf.predict(X_test_tf), results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad9fCw78cT4z"
      },
      "outputs": [],
      "source": [
        "count_vec_bin = CountVectorizer(binary=True, max_features=10000)\n",
        "X_train_bin = count_vec_bin.fit_transform(X_train)\n",
        "X_test_bin = count_vec_bin.transform(X_test)\n",
        "idf = TfidfTransformer(use_idf=True, norm=None)\n",
        "X_train_idf = idf.fit_transform(X_train_bin)\n",
        "X_test_idf = idf.transform(X_test_bin)\n",
        "\n",
        "rf_idf, params_idf = tune_rf(X_train_idf, y_train, \"IDF\")\n",
        "evaluate(\"IDF\", y_test, rf_idf.predict(X_test_idf), results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcfqzEWGcVDE"
      },
      "outputs": [],
      "source": [
        "tfidf_uni = TfidfVectorizer(ngram_range=(1,1))\n",
        "X_train_uni = tfidf_uni.fit_transform(X_train)\n",
        "X_test_uni = tfidf_uni.transform(X_test)\n",
        "\n",
        "rf_uni, params_uni = tune_rf(X_train_uni, y_train, \"TF-IDF unigram\")\n",
        "evaluate(\"TF-IDF unigram\", y_test, rf_uni.predict(X_test_uni), results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juQn5eW_cWD3"
      },
      "outputs": [],
      "source": [
        "train_tokens = [t.split() for t in X_train]\n",
        "test_tokens = [t.split() for t in X_test]\n",
        "\n",
        "w2v = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, sg=0, epochs=5)\n",
        "def avg_vec(tokens, model):\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.wv.vector_size)\n",
        "\n",
        "X_train_w2v = np.vstack([avg_vec(t, w2v) for t in train_tokens])\n",
        "X_test_w2v = np.vstack([avg_vec(t, w2v) for t in test_tokens])\n",
        "\n",
        "rf_w2v, params_w2v = tune_rf(X_train_w2v, y_train, \"Word2Vec CBOW\")\n",
        "evaluate(\"Word2Vec CBOW\", y_test, rf_w2v.predict(X_test_w2v), results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7duwuj32cXXQ"
      },
      "outputs": [],
      "source": [
        "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "X_train_st = st_model.encode(X_train, batch_size=64, convert_to_numpy=True)\n",
        "X_test_st = st_model.encode(X_test, batch_size=64, convert_to_numpy=True)\n",
        "\n",
        "rf_st, params_st = tune_rf(X_train_st, y_train, \"BERT embeddings\")\n",
        "evaluate(\"BERT embeddings\", y_test, rf_st.predict(X_test_st), results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y11UJr6KcYhi",
        "outputId": "d5e769ad-3261-4152-b5f1-f610e4681fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Final Comparison ===\n",
            "      Embedding  Accuracy  Precision  Recall       F1\n",
            "            IDF   0.83280   0.810633 0.86848 0.838560\n",
            " TF-IDF unigram   0.82152   0.807263 0.84472 0.825567\n",
            "             TF   0.82040   0.807793 0.84088 0.824004\n",
            "BERT embeddings   0.76612   0.760473 0.77696 0.768628\n",
            "  Word2Vec CBOW   0.75896   0.757395 0.76200 0.759691\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(results).sort_values(by=\"F1\", ascending=False)\n",
        "print(\"\\n=== Final Comparison ===\")\n",
        "print(df.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}