{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Download the aclImdb dataset\n",
        "!wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "XM0hJHLYlMdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim sentence_transformers"
      ],
      "metadata": {
        "id": "qtJkaPBylN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and logging\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Clean logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ensure reproducibility (basic)\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ],
      "metadata": {
        "id": "RWEzC0eolgjK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/aclImdb\"  # update if needed\n",
        "\n",
        "def read_reviews(base_dir: str) -> Tuple[List[str], List[int], List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Reads IMDB reviews keeping the original split: train/test.\n",
        "    Labels: pos=1, neg=0.\n",
        "    \"\"\"\n",
        "    def read_folder(path: str, label: int):\n",
        "        texts, labels = [], []\n",
        "        for fname in sorted(os.listdir(path)):\n",
        "            fpath = os.path.join(path, fname)\n",
        "            if os.path.isfile(fpath):\n",
        "                with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    texts.append(f.read())\n",
        "                    labels.append(label)\n",
        "        return texts, labels\n",
        "\n",
        "    train_pos, y_train_pos = read_folder(os.path.join(base_dir, \"train\", \"pos\"), 1)\n",
        "    train_neg, y_train_neg = read_folder(os.path.join(base_dir, \"train\", \"neg\"), 0)\n",
        "    test_pos, y_test_pos = read_folder(os.path.join(base_dir, \"test\", \"pos\"), 1)\n",
        "    test_neg, y_test_neg = read_folder(os.path.join(base_dir, \"test\", \"neg\"), 0)\n",
        "\n",
        "    X_train = train_pos + train_neg\n",
        "    y_train = y_train_pos + y_train_neg\n",
        "    X_test = test_pos + test_neg\n",
        "    y_test = y_test_pos + y_test_neg\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "logger.info(\"Loading dataset...\")\n",
        "X_train_raw, y_train, X_test_raw, y_test = read_reviews(DATA_DIR)\n",
        "logger.info(f\"Loaded train={len(X_train_raw)}, test={len(X_test_raw)}\")\n"
      ],
      "metadata": {
        "id": "MM4VBz6klroy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning: remove HTML, lowercase, remove punctuation, simple tokenization by whitespace\n",
        "HTML_RE = re.compile(r\"<.*?>\")\n",
        "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = HTML_RE.sub(\" \", text)\n",
        "    text = text.lower()\n",
        "    text = text.translate(PUNCT_TABLE)\n",
        "    tokens = text.split()\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "logger.info(\"Cleaning texts...\")\n",
        "X_train = [clean_text(t) for t in X_train_raw]\n",
        "X_test = [clean_text(t) for t in X_test_raw]\n"
      ],
      "metadata": {
        "id": "n4156AWQluYk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer: build vocabulary on training set for word-index sequences\n",
        "VOCAB_SIZE = 50000  # cap vocab for stability\n",
        "MAX_LEN = 128       # LSTM sequence length (truncate/pad to this)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "def to_sequences(texts: List[str], max_len: int = MAX_LEN) -> np.ndarray:\n",
        "    seqs = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "# Metrics and confusion matrix\n",
        "def evaluate_and_log(name: str, y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    logger.info(f\"[{name}] Test Accuracy={acc:.4f} | Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f}\")\n",
        "    logger.info(f\"[{name}] Confusion matrix:\\n{cm}\")\n",
        "    return {\"Embedding\": name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}\n",
        "\n",
        "# Simple LSTM model builder (Keras)\n",
        "def build_lstm(input_dim: int, embed_dim: int, lstm_hidden: int, num_layers: int, dropout: float,\n",
        "               embedding_matrix: np.ndarray = None, trainable_embed: bool = True,\n",
        "               time_steps: int = MAX_LEN, feature_dim: int = None):\n",
        "    \"\"\"\n",
        "    Builds an LSTM classifier.\n",
        "    Two modes:\n",
        "      - Token index input + Embedding layer (use input_dim & embedding_matrix)\n",
        "      - Precomputed feature sequences (use feature_dim and skip Embedding)\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(time_steps,)) if feature_dim is None else layers.Input(shape=(time_steps, feature_dim))\n",
        "\n",
        "    if feature_dim is None:\n",
        "        # Token index input with Embedding\n",
        "        if embedding_matrix is not None:\n",
        "            emb = layers.Embedding(input_dim=input_dim, output_dim=embed_dim,\n",
        "                                   weights=[embedding_matrix], trainable=trainable_embed, mask_zero=True)(inputs)\n",
        "        else:\n",
        "            emb = layers.Embedding(input_dim=input_dim, output_dim=embed_dim, trainable=trainable_embed, mask_zero=True)(inputs)\n",
        "        x = emb\n",
        "    else:\n",
        "        # Feature sequences provided (e.g., tf-idf per token, BERT chunk features)\n",
        "        x = inputs\n",
        "\n",
        "    # Stack LSTM layers\n",
        "    for i in range(num_layers):\n",
        "        return_sequences = (i < num_layers - 1)\n",
        "        x = layers.LSTM(lstm_hidden, return_sequences=return_sequences)(x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = models.Model(inputs, out)\n",
        "    return model\n",
        "\n",
        "# Training loop with logging (per epoch)\n",
        "def train_lstm(model, X_train_data, y_train, X_val_data, y_val,\n",
        "               lr=1e-3, batch_size=64, epochs=5, name=\"model\"):\n",
        "    logger.info(f\"[{name}] Training start: {datetime.now()}\")\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    hist = model.fit(\n",
        "        X_train_data, np.array(y_train),\n",
        "        validation_data=(X_val_data, np.array(y_val)),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)],\n",
        "        verbose=1\n",
        "    )\n",
        "    logger.info(f\"[{name}] Training end: {datetime.now()}\")\n",
        "    return hist\n"
      ],
      "metadata": {
        "id": "u29A-VjSlwJ8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build sequences for token-index inputs (used by W2V and pretrained embeddings)\n",
        "X_train_seq = to_sequences(X_train, MAX_LEN)\n",
        "X_test_seq = to_sequences(X_test, MAX_LEN)\n",
        "\n",
        "# Build a small validation split from training data (stratified would be ideal; simple split here)\n",
        "VAL_RATIO = 0.1\n",
        "n_train = len(X_train)\n",
        "n_val = int(n_train * VAL_RATIO)\n",
        "\n",
        "X_val_seq = X_train_seq[:n_val]\n",
        "y_val = y_train[:n_val]\n",
        "X_train_seq_model = X_train_seq[n_val:]\n",
        "y_train_model = y_train[n_val:]\n"
      ],
      "metadata": {
        "id": "R9pNF_lglzby"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF-IDF for vocabulary terms, then map to per-token weights in sequence order.\n",
        "tfidf_uni_vec = TfidfVectorizer(ngram_range=(1,1), max_features=VOCAB_SIZE)\n",
        "tfidf_uni_vec.fit(X_train)\n",
        "\n",
        "# Build per-token tf-idf weight sequences\n",
        "def tfidf_sequence(texts, tokenizer, vectorizer, max_len=MAX_LEN):\n",
        "    # Map token -> tf-idf idf weight (global). We'll use IDF for per-token static weight.\n",
        "    idf_map = {t: vectorizer.idf_[i] for t, i in vectorizer.vocabulary_.items()}\n",
        "    seqs = tokenizer.texts_to_sequences(texts)\n",
        "    feats = []\n",
        "    for seq, txt in zip(seqs, texts):\n",
        "        tokens = txt.split()\n",
        "        # Align indices in sequence to token list length\n",
        "        weights = []\n",
        "        ti = 0\n",
        "        for idx in seq[:max_len]:\n",
        "            # find the original token string; fallback to zero if out-of-range\n",
        "            tok = tokens[ti] if ti < len(tokens) else \"\"\n",
        "            ti += 1\n",
        "            w = idf_map.get(tok, 0.0)\n",
        "            weights.append([w])\n",
        "        # pad\n",
        "        while len(weights) < max_len:\n",
        "            weights.append([0.0])\n",
        "        feats.append(weights)\n",
        "    return np.array(feats, dtype=np.float32)\n",
        "\n",
        "X_train_tfidf_uni_feat_all = tfidf_sequence(X_train, tokenizer, tfidf_uni_vec, MAX_LEN)\n",
        "X_test_tfidf_uni_feat = tfidf_sequence(X_test, tokenizer, tfidf_uni_vec, MAX_LEN)\n",
        "\n",
        "# Create val split aligned with earlier split\n",
        "X_val_tfidf_uni_feat = X_train_tfidf_uni_feat_all[:n_val]\n",
        "X_train_tfidf_uni_feat = X_train_tfidf_uni_feat_all[n_val:]\n",
        "\n",
        "# Hyperparameters to try (small grid)\n",
        "grid_tfidf_uni = [\n",
        "    {\"lstm_hidden\": 64, \"num_layers\": 1, \"dropout\": 0.2, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
        "    {\"lstm_hidden\": 128, \"num_layers\": 1, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
        "]\n",
        "\n",
        "best_val_acc_tfidf_uni = 0.0\n",
        "best_params_tfidf_uni = None\n",
        "best_model_tfidf_uni = None\n",
        "\n",
        "for p in grid_tfidf_uni:\n",
        "    name = f\"TF-IDF(unigram)_LSTM_{p}\"\n",
        "    logger.info(f\"=== Training {name} ===\")\n",
        "    model = build_lstm(\n",
        "        input_dim=None, embed_dim=None, lstm_hidden=p[\"lstm_hidden\"], num_layers=p[\"num_layers\"],\n",
        "        dropout=p[\"dropout\"], embedding_matrix=None, trainable_embed=False,\n",
        "        time_steps=MAX_LEN, feature_dim=1  # feature sequence: 1-d per token\n",
        "    )\n",
        "    hist = train_lstm(model, X_train_tfidf_uni_feat, y_train_model, X_val_tfidf_uni_feat, y_val,\n",
        "                      lr=p[\"lr\"], batch_size=p[\"batch_size\"], epochs=p[\"epochs\"], name=name)\n",
        "    val_acc = max(hist.history[\"val_accuracy\"])\n",
        "    if val_acc > best_val_acc_tfidf_uni:\n",
        "        best_val_acc_tfidf_uni = val_acc\n",
        "        best_params_tfidf_uni = p\n",
        "        best_model_tfidf_uni = model\n",
        "\n",
        "logger.info(f\"[TF-IDF(unigram)] Best val accuracy: {best_val_acc_tfidf_uni:.4f} | Params: {best_params_tfidf_uni}\")\n",
        "\n",
        "# Test evaluation\n",
        "y_pred_prob = best_model_tfidf_uni.predict(X_test_tfidf_uni_feat, batch_size=128).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "metrics_tfidf_uni = evaluate_and_log(\"TF-IDF (unigram) seq\", y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "jKRUGcMHl05M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build bigram-augmented texts\n",
        "def add_bigrams(texts):\n",
        "    augmented = []\n",
        "    for t in texts:\n",
        "        toks = t.split()\n",
        "        bigrams = [\"{}_{}\".format(toks[i], toks[i+1]) for i in range(len(toks)-1)]\n",
        "        augmented.append(t + \" \" + \" \".join(bigrams))\n",
        "    return augmented\n",
        "\n",
        "X_train_aug = add_bigrams(X_train)\n",
        "X_test_aug = add_bigrams(X_test)\n",
        "\n",
        "# Fit TF-IDF on augmented text (unigram+bigram)\n",
        "tfidf_unibi_vec = TfidfVectorizer(ngram_range=(1,2), max_features=VOCAB_SIZE)\n",
        "tfidf_unibi_vec.fit(X_train_aug)\n",
        "\n",
        "# Build sequences using original tokenizer indexes but feature from augmented vocabulary IDF\n",
        "def tfidf_unibi_sequence(texts, tokenizer, vectorizer, max_len=MAX_LEN):\n",
        "    idf_map = {t: vectorizer.idf_[i] for t, i in vectorizer.vocabulary_.items()}\n",
        "    seqs = tokenizer.texts_to_sequences(texts)\n",
        "    feats = []\n",
        "    for seq, txt in zip(seqs, texts):\n",
        "        tokens = txt.split()\n",
        "        weights = []\n",
        "        ti = 0\n",
        "        for idx in seq[:max_len]:\n",
        "            tok = tokens[ti] if ti < len(tokens) else \"\"\n",
        "            ti += 1\n",
        "            # try unigram IDF first; bigram signal is indirectly captured via augmented fitting\n",
        "            w = idf_map.get(tok, 0.0)\n",
        "            weights.append([w])\n",
        "        while len(weights) < max_len:\n",
        "            weights.append([0.0])\n",
        "        feats.append(weights)\n",
        "    return np.array(feats, dtype=np.float32)\n",
        "\n",
        "X_train_tfidf_unibi_feat_all = tfidf_unibi_sequence(X_train, tokenizer, tfidf_unibi_vec, MAX_LEN)\n",
        "X_test_tfidf_unibi_feat = tfidf_unibi_sequence(X_test, tokenizer, tfidf_unibi_vec, MAX_LEN)\n",
        "X_val_tfidf_unibi_feat = X_train_tfidf_unibi_feat_all[:n_val]\n",
        "X_train_tfidf_unibi_feat = X_train_tfidf_unibi_feat_all[n_val:]\n",
        "\n",
        "grid_tfidf_unibi = [\n",
        "    {\"lstm_hidden\": 64, \"num_layers\": 1, \"dropout\": 0.2, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
        "    {\"lstm_hidden\": 128, \"num_layers\": 1, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
        "]\n",
        "\n",
        "best_val_acc_tfidf_unibi = 0.0\n",
        "best_params_tfidf_unibi = None\n",
        "best_model_tfidf_unibi = None\n",
        "\n",
        "for p in grid_tfidf_unibi:\n",
        "    name = f\"TF-IDF(unigram+bigram)_LSTM_{p}\"\n",
        "    logger.info(f\"=== Training {name} ===\")\n",
        "    model = build_lstm(\n",
        "        input_dim=None, embed_dim=None, lstm_hidden=p[\"lstm_hidden\"], num_layers=p[\"num_layers\"],\n",
        "        dropout=p[\"dropout\"], embedding_matrix=None, trainable_embed=False,\n",
        "        time_steps=MAX_LEN, feature_dim=1\n",
        "    )\n",
        "    hist = train_lstm(model, X_train_tfidf_unibi_feat, y_train_model, X_val_tfidf_unibi_feat, y_val,\n",
        "                      lr=p[\"lr\"], batch_size=p[\"batch_size\"], epochs=p[\"epochs\"], name=name)\n",
        "    val_acc = max(hist.history[\"val_accuracy\"])\n",
        "    if val_acc > best_val_acc_tfidf_unibi:\n",
        "        best_val_acc_tfidf_unibi = val_acc\n",
        "        best_params_tfidf_unibi = p\n",
        "        best_model_tfidf_unibi = model\n",
        "\n",
        "logger.info(f\"[TF-IDF(unigram+bigram)] Best val accuracy: {best_val_acc_tfidf_unibi:.4f} | Params: {best_params_tfidf_unibi}\")\n",
        "\n",
        "y_pred_prob = best_model_tfidf_unibi.predict(X_test_tfidf_unibi_feat, batch_size=128).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "metrics_tfidf_unibi = evaluate_and_log(\"TF-IDF (unigram+bigram) seq\", y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "wTBRCNZml3OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec (CBOW)\n",
        "train_tokens = [t.split() for t in X_train]\n",
        "w2v_dim = 100\n",
        "logger.info(\"[Word2Vec CBOW] Training gensim Word2Vec...\")\n",
        "w2v_cbow = Word2Vec(sentences=train_tokens, vector_size=w2v_dim, window=5, min_count=2, workers=os.cpu_count(), sg=0, epochs=5)\n",
        "\n",
        "# Build embedding matrix aligned to Keras tokenizer index\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
        "embedding_matrix_w2v = np.random.normal(scale=0.01, size=(num_words, w2v_dim)).astype(np.float32)\n",
        "\n",
        "for word, idx in word_index.items():\n",
        "    if idx >= num_words:\n",
        "        continue\n",
        "    if word in w2v_cbow.wv:\n",
        "        embedding_matrix_w2v[idx] = w2v_cbow.wv[word]\n",
        "\n",
        "# Hyperparameter grid\n",
        "grid_w2v = [\n",
        "    {\"embed_dim\": w2v_dim, \"lstm_hidden\": 128, \"num_layers\": 1, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4, \"trainable_embed\": True},\n",
        "    {\"embed_dim\": w2v_dim, \"lstm_hidden\": 128, \"num_layers\": 2, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4, \"trainable_embed\": True},\n",
        "]\n",
        "\n",
        "best_val_acc_w2v = 0.0\n",
        "best_params_w2v = None\n",
        "best_model_w2v = None\n",
        "\n",
        "for p in grid_w2v:\n",
        "    name = f\"Word2Vec(trainable)_LSTM_{p}\"\n",
        "    logger.info(f\"=== Training {name} ===\")\n",
        "    model = build_lstm(\n",
        "        input_dim=num_words, embed_dim=p[\"embed_dim\"], lstm_hidden=p[\"lstm_hidden\"], num_layers=p[\"num_layers\"],\n",
        "        dropout=p[\"dropout\"], embedding_matrix=embedding_matrix_w2v, trainable_embed=p[\"trainable_embed\"],\n",
        "        time_steps=MAX_LEN\n",
        "    )\n",
        "    hist = train_lstm(model, X_train_seq_model, y_train_model, X_val_seq, y_val,\n",
        "                      lr=p[\"lr\"], batch_size=p[\"batch_size\"], epochs=p[\"epochs\"], name=name)\n",
        "    val_acc = max(hist.history[\"val_accuracy\"])\n",
        "    if val_acc > best_val_acc_w2v:\n",
        "        best_val_acc_w2v = val_acc\n",
        "        best_params_w2v = p\n",
        "        best_model_w2v = model\n",
        "\n",
        "logger.info(f\"[Word2Vec(trainable)] Best val accuracy: {best_val_acc_w2v:.4f} | Params: {best_params_w2v}\")\n",
        "\n",
        "y_pred_prob = best_model_w2v.predict(X_test_seq, batch_size=128).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "metrics_w2v = evaluate_and_log(\"Word2Vec (trainable)\", y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "qrijGSGvl5Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained embeddings\n",
        "try:\n",
        "    kv = api.load(\"word2vec-google-news-300\")  # large (~1.5GB)\n",
        "    pre_dim = 300\n",
        "    pre_name = \"Pretrained Word2Vec (GoogleNews)\"\n",
        "except Exception:\n",
        "    kv = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "    pre_dim = 300\n",
        "    pre_name = \"Pretrained FastText (WikiNews)\"\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
        "embedding_matrix_pre = np.random.normal(scale=0.01, size=(num_words, pre_dim)).astype(np.float32)\n",
        "\n",
        "for word, idx in word_index.items():\n",
        "    if idx >= num_words:\n",
        "        continue\n",
        "    if word in kv:\n",
        "        embedding_matrix_pre[idx] = kv[word]\n",
        "\n",
        "grid_pre = [\n",
        "    {\"embed_dim\": pre_dim, \"lstm_hidden\": 128, \"num_layers\": 1, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4, \"trainable_embed\": False},\n",
        "    {\"embed_dim\": pre_dim, \"lstm_hidden\": 128, \"num_layers\": 2, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4, \"trainable_embed\": False},\n",
        "]\n",
        "\n",
        "best_val_acc_pre = 0.0\n",
        "best_params_pre = None\n",
        "best_model_pre = None\n",
        "\n",
        "for p in grid_pre:\n",
        "    name = f\"{pre_name}_LSTM_{p}\"\n",
        "    logger.info(f\"=== Training {name} ===\")\n",
        "    model = build_lstm(\n",
        "        input_dim=num_words, embed_dim=p[\"embed_dim\"], lstm_hidden=p[\"lstm_hidden\"], num_layers=p[\"num_layers\"],\n",
        "        dropout=p[\"dropout\"], embedding_matrix=embedding_matrix_pre, trainable_embed=p[\"trainable_embed\"],\n",
        "        time_steps=MAX_LEN\n",
        "    )\n",
        "    hist = train_lstm(model, X_train_seq_model, y_train_model, X_val_seq, y_val,\n",
        "                      lr=p[\"lr\"], batch_size=p[\"batch_size\"], epochs=p[\"epochs\"], name=name)\n",
        "    val_acc = max(hist.history[\"val_accuracy\"])\n",
        "    if val_acc > best_val_acc_pre:\n",
        "        best_val_acc_pre = val_acc\n",
        "        best_params_pre = p\n",
        "        best_model_pre = model\n",
        "\n",
        "logger.info(f\"[{pre_name}] Best val accuracy: {best_val_acc_pre:.4f} | Params: {best_params_pre}\")\n",
        "\n",
        "y_pred_prob = best_model_pre.predict(X_test_seq, batch_size=128).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "metrics_pre = evaluate_and_log(pre_name, y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "Ph9GJKhMl69r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk helper: split review into fixed-size word chunks\n",
        "def chunk_text(text: str, chunk_size: int = 32, max_chunks: int = 8):\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, min(len(tokens), chunk_size * max_chunks), chunk_size):\n",
        "        chunk = \" \".join(tokens[i:i+chunk_size])\n",
        "        if len(chunk) > 0:\n",
        "            chunks.append(chunk)\n",
        "    # pad with empty chunks\n",
        "    while len(chunks) < max_chunks:\n",
        "        chunks.append(\"\")\n",
        "    return chunks[:max_chunks]\n",
        "\n",
        "# Build chunked sequences\n",
        "CHUNK_SIZE = 32\n",
        "MAX_CHUNKS = 8  # sequence length for LSTM in this experiment\n",
        "logger.info(\"[BERT] Preparing chunked inputs...\")\n",
        "X_train_chunks = [chunk_text(t, CHUNK_SIZE, MAX_CHUNKS) for t in X_train]\n",
        "X_test_chunks = [chunk_text(t, CHUNK_SIZE, MAX_CHUNKS) for t in X_test]\n",
        "X_val_chunks = X_train_chunks[:n_val]\n",
        "X_train_chunks_model = X_train_chunks[n_val:]\n",
        "\n",
        "# Encode chunks with sentence-transformers (each chunk -> 384-d for all-MiniLM-L6-v2)\n",
        "st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "logger.info(\"[BERT] Encoding chunks for train/val/test...\")\n",
        "def encode_chunks(chunk_lists):\n",
        "    # Flatten to encode in batches, then reshape back\n",
        "    flat = [c for chunks in chunk_lists for c in chunks]\n",
        "    embs = st.encode(flat, batch_size=128, convert_to_numpy=True, show_progress_bar=True)\n",
        "    # reshape: (num_docs, MAX_CHUNKS, emb_dim)\n",
        "    emb_dim = embs.shape[1]\n",
        "    arr = embs.reshape((len(chunk_lists), MAX_CHUNKS, emb_dim))\n",
        "    return arr, emb_dim\n",
        "\n",
        "X_train_bert_seq, bert_dim = encode_chunks(X_train_chunks_model)\n",
        "X_val_bert_seq, _ = encode_chunks(X_val_chunks)\n",
        "X_test_bert_seq, _ = encode_chunks(X_test_chunks)\n",
        "\n",
        "grid_bert = [\n",
        "    {\"lstm_hidden\": 128, \"num_layers\": 1, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 4},\n",
        "    {\"lstm_hidden\": 128, \"num_layers\": 2, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 4},\n",
        "]\n",
        "\n",
        "best_val_acc_bert = 0.0\n",
        "best_params_bert = None\n",
        "best_model_bert = None\n",
        "\n",
        "for p in grid_bert:\n",
        "    name = f\"BERT(chunks)_LSTM_{p}\"\n",
        "    logger.info(f\"=== Training {name} ===\")\n",
        "    model = build_lstm(\n",
        "        input_dim=None, embed_dim=None, lstm_hidden=p[\"lstm_hidden\"], num_layers=p[\"num_layers\"],\n",
        "        dropout=p[\"dropout\"], embedding_matrix=None, trainable_embed=False,\n",
        "        time_steps=MAX_CHUNKS, feature_dim=bert_dim  # feature sequences (chunk embeddings)\n",
        "    )\n",
        "    hist = train_lstm(model, X_train_bert_seq, y_train_model, X_val_bert_seq, y_val,\n",
        "                      lr=p[\"lr\"], batch_size=p[\"batch_size\"], epochs=p[\"epochs\"], name=name)\n",
        "    val_acc = max(hist.history[\"val_accuracy\"])\n",
        "    if val_acc > best_val_acc_bert:\n",
        "        best_val_acc_bert = val_acc\n",
        "        best_params_bert = p\n",
        "        best_model_bert = model\n",
        "\n",
        "logger.info(f\"[BERT(chunks)] Best val accuracy: {best_val_acc_bert:.4f} | Params: {best_params_bert}\")\n",
        "\n",
        "y_pred_prob = best_model_bert.predict(X_test_bert_seq, batch_size=128).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "metrics_bert = evaluate_and_log(\"BERT chunked embeddings\", y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "pX4gXZNPl8WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect metrics and best params\n",
        "results = [\n",
        "    metrics_tfidf_uni,\n",
        "    metrics_tfidf_unibi,\n",
        "    metrics_w2v,\n",
        "    metrics_pre,\n",
        "    metrics_bert\n",
        "]\n",
        "\n",
        "best_params = [\n",
        "    {\"Embedding\": \"TF-IDF (unigram) seq\", \"Best Val Acc\": float(f\"{best_val_acc_tfidf_uni:.4f}\"), \"Best Params\": best_params_tfidf_uni},\n",
        "    {\"Embedding\": \"TF-IDF (unigram+bigram) seq\", \"Best Val Acc\": float(f\"{best_val_acc_tfidf_unibi:.4f}\"), \"Best Params\": best_params_tfidf_unibi},\n",
        "    {\"Embedding\": \"Word2Vec (trainable)\", \"Best Val Acc\": float(f\"{best_val_acc_w2v:.4f}\"), \"Best Params\": best_params_w2v},\n",
        "    {\"Embedding\": f\"{pre_name}\", \"Best Val Acc\": float(f\"{best_val_acc_pre:.4f}\"), \"Best Params\": best_params_pre},\n",
        "    {\"Embedding\": \"BERT chunked embeddings\", \"Best Val Acc\": float(f\"{best_val_acc_bert:.4f}\"), \"Best Params\": best_params_bert},\n",
        "]\n",
        "\n",
        "df_metrics = pd.DataFrame(results)\n",
        "df_params = pd.DataFrame(best_params)\n",
        "\n",
        "# Merge and present key columns\n",
        "df_final = df_params[[\"Embedding\", \"Best Val Acc\", \"Best Params\"]].merge(\n",
        "    df_metrics[[\"Embedding\", \"Accuracy\", \"F1\"]],\n",
        "    on=\"Embedding\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "df_final = df_final.sort_values(by=\"F1\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== Final Comparison (sorted by Test F1) ===\")\n",
        "print(df_final.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jx8J7KUl95V",
        "outputId": "625f2ee6-effa-4273-9c00-4150d52e6d60"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Final Comparison (sorted by Test F1) ===\n",
            "                       Embedding  Best Val Acc                                                                                                                                   Best Params  Accuracy       F1\n",
            "         BERT chunked embeddings        0.8116                                             {'lstm_hidden': 128, 'num_layers': 2, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 32, 'epochs': 4}   0.83896 0.836766\n",
            "            Word2Vec (trainable)        0.7824  {'embed_dim': 200, 'lstm_hidden': 128, 'num_layers': 1, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 64, 'epochs': 4, 'trainable_embed': True}   0.82064 0.809256\n",
            "Pretrained Word2Vec (GoogleNews)        0.8916 {'embed_dim': 300, 'lstm_hidden': 128, 'num_layers': 2, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 64, 'epochs': 4, 'trainable_embed': False}   0.65964 0.725188\n",
            "     TF-IDF (unigram+bigram) seq        0.0228                                              {'lstm_hidden': 64, 'num_layers': 1, 'dropout': 0.2, 'lr': 0.001, 'batch_size': 64, 'epochs': 4}   0.50048 0.041008\n",
            "            TF-IDF (unigram) seq        0.0052                                              {'lstm_hidden': 64, 'num_layers': 1, 'dropout': 0.2, 'lr': 0.001, 'batch_size': 64, 'epochs': 4}   0.50028 0.004621\n"
          ]
        }
      ]
    }
  ]
}